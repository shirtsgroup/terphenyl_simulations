                      :-) GROMACS - gmx mdrun, 2018.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar    Aldert van Buuren   Rudi van Drunen     Anton Feenstra  
  Gerrit Groenhof    Aleksei Iupinov   Christoph Junghans   Anca Hamuraru   
 Vincent Hindriksen Dimitrios Karkoulis    Peter Kasson        Jiri Kraus    
  Carsten Kutzner      Per Larsson      Justin A. Lemkul    Viveca Lindahl  
  Magnus Lundborg   Pieter Meulenhoff    Erik Marklund      Teemu Murtola   
    Szilard Pall       Sander Pronk      Roland Schulz     Alexey Shvetsov  
   Michael Shirts     Alfons Sijbers     Peter Tieleman    Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2018.1
Executable:   /projects/beco4952/pkgs/gromacs/2018_gpu/bin/gmx_mpi
Data prefix:  /projects/beco4952/pkgs/gromacs/2018_gpu
Working dir:  /gpfs/summit/scratch/thfo9888/heteropolymers/octamer_Rchiral/RHH/remd_sim
Command line:
  gmx_mpi mdrun -v -multidir sim0 sim1 sim2 sim3 sim4 sim5 sim6 sim7 sim8 sim9 sim10 sim11 sim12 sim13 sim14 sim15 sim16 sim17 sim18 sim19 -deffnm npt -ntomp 6 -nex 8000 -replex 100


Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#

Back Off! I just backed up npt.log to ./#npt.log.22#
Compiled SIMD: SSE4.1, but for this host/run AVX2_256 might be better (see
log).
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Reading file npt.tpr, VERSION 2018.1 (single precision)
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Reading file npt.tpr, VERSION 2018.1 (single precision)
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Reading file npt.tpr, VERSION 2018.1 (single precision)
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Reading file npt.tpr, VERSION 2018.1 (single precision)
Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1

Changing nstlist from 40 to 100, rlist from 1 to 1


This is simulation 8 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 9 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 11 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 16 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 18 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 4 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 0 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 5 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 1 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 6 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 2 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 7 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 3 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 12 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 10 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 13 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 17 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 14 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 19 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 15 out of 20 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

Using 6 OpenMP threads 

4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 8 GPU tasks in the 8 ranks on this node:
  PP:0,PP:0,PP:1,PP:1,PP:2,PP:2,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3
4 GPUs auto-selected for this run.
Mapping of GPU IDs to the 12 GPU tasks in the 12 ranks on this node:
  PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:2,PP:2,PP:2,PP:3,PP:3,PP:3

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 28 logical CPU cores per node with 48 threads.
         This will cause considerable performance loss.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

NOTE: GROMACS was configured without NVML support hence it can not exploit
      application clocks of the detected Tesla K80 GPU to improve performance.
      Recompile with the NVML library (compatible with the driver used) or set application clocks manually.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

WARNING: On rank 0: oversubscribing the available 24 logical CPU cores per node with 72 threads.
         This will cause considerable performance loss.

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#

Back Off! I just backed up npt.trr to ./#npt.trr.7#

Back Off! I just backed up npt.edr to ./#npt.edr.7#
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
starting mdrun 'Simulation Box in water'
7500000 steps,  15000.0 ps.
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1756.5 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1808.1 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1748.0 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1285.2 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1753.6 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1383.4 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1346.9 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1356.6 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1143.7 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1214.7 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1418.0 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1915.3 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1375.1 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1430.4 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1680.8 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 2004.4 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 2229.5 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1947.8 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1216.6 M-cycles
step  200: timed with pme grid 48 48 48, coulomb cutoff 1.000: 2242.2 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.104: 944.4 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.084: 1421.0 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.094: 1396.5 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.111: 1411.9 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.089: 1403.8 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.093: 1421.6 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.119: 920.3 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.099: 928.3 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.080: 946.6 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.095: 952.3 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.085: 950.3 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.080: 963.7 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.118: 1490.6 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.076: 1520.9 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.097: 1725.6 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.095: 1860.6 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.108: 1932.3 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.117: 1985.7 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.123: 1984.9 M-cycles
step  400: timed with pme grid 42 42 42, coulomb cutoff 1.103: 1935.5 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.271: 1429.3 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.276: 1429.8 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.288: 1455.1 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.293: 1325.4 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.275: 1414.0 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.265: 1655.0 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.303: 973.6 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.260: 1300.7 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.306: 1542.4 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.256: 1536.9 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.282: 1468.7 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.265: 1489.2 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.260: 1471.0 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.296: 1513.6 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.304: 1553.1 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.287: 1754.5 M-cycles
step  600: timed with pme grid 36 36 36, coulomb cutoff 1.277: 1520.4 M-cycles
step  600: timed with pme grid 44 44 44, coulomb cutoff 1.045: 1011.4 M-cycles
              optimal pme grid 44 44 44, coulomb cutoff 1.045
step  600: timed with pme grid 44 44 44, coulomb cutoff 1.047: 1018.7 M-cycles
              optimal pme grid 44 44 44, coulomb cutoff 1.047
step  600: timed with pme grid 44 44 44, coulomb cutoff 1.072: 979.9 M-cycles
              optimal pme grid 44 44 44, coulomb cutoff 1.072
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.454: 1300.8 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.454
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.434: 1269.1 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.434
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.436: 1292.3 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.436
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.139: 1303.4 M-cycles
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.430: 1292.7 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.430
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.159: 1008.2 M-cycles
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.447: 1300.6 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.447
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.413: 1369.8 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.413
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.468: 1379.0 M-cycles
step  800: the maximum allowed grid scaling limits the PME load balancing to a coulomb cut-off of 1.468
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.166: 1486.8 M-cycles
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.175: 759.6 M-cycles
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.154: 793.6 M-cycles
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.134: 806.8 M-cycles
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.139: 786.9 M-cycles
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.134: 817.1 M-cycles
step  800: timed with pme grid 40 40 40, coulomb cutoff 1.150: 799.4 M-cycles
step  800: timed with pme grid 32 32 32, coulomb cutoff 1.466: 1888.5 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.436: 1209.4 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.454: 1238.3 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.430: 708.6 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.434: 1380.0 M-cycles
step 1000: timed with pme grid 42 42 42, coulomb cutoff 1.111: 990.9 M-cycles
step 1000: timed with pme grid 42 42 42, coulomb cutoff 1.084: 1230.0 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.468: 1206.1 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.413: 1213.4 M-cycles
step 1000: timed with pme grid 32 32 32, coulomb cutoff 1.447: 1416.3 M-cycles
              optimal pme grid 32 32 32, coulomb cutoff 1.447
step 1000: timed with pme grid 36 36 36, coulomb cutoff 1.303: 1336.5 M-cycles
step 1000: timed with pme grid 42 42 42, coulomb cutoff 1.104: 1602.3 M-cycles
step 1000: timed with pme grid 44 44 44, coulomb cutoff 1.068: 1775.4 M-cycles
              optimal pme grid 40 40 40, coulomb cutoff 1.175
step 1000: timed with pme grid 44 44 44, coulomb cutoff 1.045: 1759.1 M-cycles
step 1000: timed with pme grid 44 44 44, coulomb cutoff 1.031: 1763.8 M-cycles
              optimal pme grid 40 40 40, coulomb cutoff 1.150
              optimal pme grid 40 40 40, coulomb cutoff 1.134
step 1000: timed with pme grid 44 44 44, coulomb cutoff 1.035: 1773.7 M-cycles
              optimal pme grid 40 40 40, coulomb cutoff 1.139
step 1000: timed with pme grid 44 44 44, coulomb cutoff 1.031: 1780.2 M-cycles
              optimal pme grid 40 40 40, coulomb cutoff 1.134
step 1000: timed with pme grid 44 44 44, coulomb cutoff 1.049: 1768.9 M-cycles
              optimal pme grid 40 40 40, coulomb cutoff 1.154
step 1200: timed with pme grid 36 36 36, coulomb cutoff 1.276: 1382.8 M-cycles
step 1200: timed with pme grid 36 36 36, coulomb cutoff 1.275: 1353.9 M-cycles
step 1200: timed with pme grid 44 44 44, coulomb cutoff 1.035: 880.4 M-cycles
step 1200: timed with pme grid 36 36 36, coulomb cutoff 1.293: 1532.7 M-cycles
step 1200: timed with pme grid 44 44 44, coulomb cutoff 1.060: 1368.6 M-cycles
step 1200: timed with pme grid 44 44 44, coulomb cutoff 1.054: 1382.3 M-cycles
              optimal pme grid 42 42 42, coulomb cutoff 1.104
step 1200: timed with pme grid 36 36 36, coulomb cutoff 1.271: 1394.3 M-cycles
step 1200: timed with pme grid 40 40 40, coulomb cutoff 1.173: 948.4 M-cycles
              optimal pme grid 40 40 40, coulomb cutoff 1.173
step 1200: timed with pme grid 36 36 36, coulomb cutoff 1.256: 1920.6 M-cycles
step 1200: timed with pme grid 36 36 36, coulomb cutoff 1.304: 1953.8 M-cycles
step 1400: timed with pme grid 40 40 40, coulomb cutoff 1.149: 1461.2 M-cycles
step 1400: timed with pme grid 40 40 40, coulomb cutoff 1.144: 1466.4 M-cycles
step 1400: timed with pme grid 40 40 40, coulomb cutoff 1.147: 1464.5 M-cycles
step 1400: timed with pme grid 44 44 44, coulomb cutoff 1.035: 1442.4 M-cycles
              optimal pme grid 44 44 44, coulomb cutoff 1.035
step 1400: timed with pme grid 48 48 48, coulomb cutoff 1.000: 1444.7 M-cycles
step 1400: timed with pme grid 40 40 40, coulomb cutoff 1.164: 1927.0 M-cycles
step 1400: timed with pme grid 40 40 40, coulomb cutoff 1.130: 2015.8 M-cycles
step 1400: timed with pme grid 40 40 40, coulomb cutoff 1.174: 2038.8 M-cycles
step 1600: timed with pme grid 32 32 32, coulomb cutoff 1.434: 541.4 M-cycles
step 1600: timed with pme grid 32 32 32, coulomb cutoff 1.454: 1026.8 M-cycles
step 1600: timed with pme grid 42 42 42, coulomb cutoff 1.094: 543.3 M-cycles
step 1600: timed with pme grid 42 42 42, coulomb cutoff 1.089: 549.3 M-cycles
step 1600: timed with pme grid 42 42 42, coulomb cutoff 1.111: 1025.6 M-cycles
              optimal pme grid 42 42 42, coulomb cutoff 1.111
step 1600: timed with pme grid 42 42 42, coulomb cutoff 1.076: 1055.3 M-cycles
step 1600: timed with pme grid 42 42 42, coulomb cutoff 1.118: 1056.4 M-cycles
step 1800: timed with pme grid 36 36 36, coulomb cutoff 1.293: 951.8 M-cycles
step 1800: timed with pme grid 36 36 36, coulomb cutoff 1.275: 1906.8 M-cycles
step 1800: timed with pme grid 44 44 44, coulomb cutoff 1.027: 1004.1 M-cycles
step 1800: timed with pme grid 44 44 44, coulomb cutoff 1.067: 1017.7 M-cycles
step 1800: timed with pme grid 44 44 44, coulomb cutoff 1.044: 1900.1 M-cycles
step 1800: timed with pme grid 44 44 44, coulomb cutoff 1.040: 1907.4 M-cycles
step 2000: timed with pme grid 40 40 40, coulomb cutoff 1.164: 1515.5 M-cycles
step 2000: timed with pme grid 40 40 40, coulomb cutoff 1.147: 1518.4 M-cycles
step 2000: timed with pme grid 42 42 42, coulomb cutoff 1.094: 1507.2 M-cycles
              optimal pme grid 42 42 42, coulomb cutoff 1.094
step 2000: timed with pme grid 42 42 42, coulomb cutoff 1.089: 1513.0 M-cycles
              optimal pme grid 42 42 42, coulomb cutoff 1.089
step 2000: timed with pme grid 42 42 42, coulomb cutoff 1.076: 1606.4 M-cycles
step 2000: timed with pme grid 42 42 42, coulomb cutoff 1.118: 1586.2 M-cycles
step 2200: timed with pme grid 32 32 32, coulomb cutoff 1.454: 1225.5 M-cycles
step 2200: timed with pme grid 32 32 32, coulomb cutoff 1.434: 1468.3 M-cycles
              optimal pme grid 32 32 32, coulomb cutoff 1.434
step 2200: timed with pme grid 44 44 44, coulomb cutoff 1.027: 1266.9 M-cycles
              optimal pme grid 44 44 44, coulomb cutoff 1.027
step 2200: timed with pme grid 44 44 44, coulomb cutoff 1.067: 1288.5 M-cycles
              optimal pme grid 44 44 44, coulomb cutoff 1.067
step 2300, will finish Sat May  2 13:57:18 2020step 2400: timed with pme grid 36 36 36, coulomb cutoff 1.293: 1534.7 M-cycles
              optimal pme grid 36 36 36, coulomb cutoff 1.293
step 2400, will finish Sat May  2 13:49:14 2020step 2500, will finish Sat May  2 13:18:02 2020step 2600, will finish Sat May  2 12:59:28 2020step 2700, will finish Sat May  2 12:32:44 2020step 2800, will finish Sat May  2 12:27:02 2020step 2900, will finish Sat May  2 12:09:48 2020step 3000, will finish Sat May  2 11:54:54 2020step 3100, will finish Sat May  2 11:46:40 2020step 3200, will finish Sat May  2 11:29:30 2020step 3300, will finish Sat May  2 11:13:58 2020step 3400, will finish Sat May  2 11:06:51 2020step 3500, will finish Sat May  2 10:48:19 2020step 3600, will finish Sat May  2 10:31:49 2020step 3700, will finish Sat May  2 10:17:06 2020step 3800, will finish Sat May  2 10:10:57 2020step 3900, will finish Sat May  2 09:59:20 2020step 4000, will finish Sat May  2 09:46:09 2020step 4100, will finish Sat May  2 09:32:46 2020step 4200, will finish Sat May  2 09:31:58 2020step 4300, will finish Sat May  2 09:22:54 2020step 4400, will finish Sat May  2 09:13:09 2020step 4500, will finish Sat May  2 09:03:34 2020step 4600, will finish Sat May  2 09:03:23 2020step 4700, will finish Sat May  2 08:53:02 2020step 4800, will finish Sat May  2 08:45:53 2020step 4900, will finish Sat May  2 08:35:34 2020step 5000, will finish Sat May  2 08:35:52 2020step 5100, will finish Sat May  2 08:31:44 2020step 5200, will finish Sat May  2 08:28:12 2020step 5300, will finish Sat May  2 08:31:28 2020step 5400, will finish Sat May  2 08:26:06 2020step 5500, will finish Sat May  2 08:17:58 2020step 5600, will finish Sat May  2 08:15:53 2020step 5700, will finish Sat May  2 08:08:48 2020step 5800, will finish Sat May  2 08:04:16 2020step 5900, will finish Sat May  2 07:56:52 2020step 6000, will finish Sat May  2 07:55:34 2020step 6100, will finish Sat May  2 07:50:56 2020step 6200, will finish Sat May  2 07:47:46 2020step 6300, will finish Sat May  2 07:42:02 2020step 6400, will finish Sat May  2 07:39:03 2020step 6500, will finish Sat May  2 07:33:47 2020step 6600, will finish Sat May  2 07:27:36 2020step 6700, will finish Sat May  2 07:22:00 2020step 6800, will finish Sat May  2 07:19:31 2020step 6900, will finish Sat May  2 07:14:07 2020step 7000, will finish Sat May  2 07:11:45 2020step 7100, will finish Sat May  2 07:06:49 2020step 7200, will finish Sat May  2 07:04:44 2020step 7300, will finish Sat May  2 06:59:59 2020step 7400, will finish Sat May  2 06:57:31 2020step 7500, will finish Sat May  2 06:53:20 2020step 7600, will finish Sat May  2 06:51:25 2020step 7700, will finish Sat May  2 06:47:10 2020step 7800, will finish Sat May  2 06:44:53 2020step 7900, will finish Sat May  2 06:40:59 2020step 8000, will finish Sat May  2 06:36:59 2020step 8100, will finish Sat May  2 06:35:39 2020step 8200, will finish Sat May  2 06:32:33 2020step 8300, will finish Sat May  2 06:28:08 2020step 8400, will finish Sat May  2 06:24:05 2020step 8500, will finish Sat May  2 06:23:04 2020step 8600, will finish Sat May  2 06:21:13 2020step 8700, will finish Sat May  2 06:17:32 2020step 8800, will finish Sat May  2 06:14:10 2020step 8900, will finish Sat May  2 06:13:37 2020step 9000, will finish Sat May  2 06:12:15 2020step 9100, will finish Sat May  2 06:08:50 2020step 9200, will finish Sat May  2 06:06:50 2020step 9300, will finish Sat May  2 06:06:02 2020step 9400, will finish Sat May  2 06:04:54 2020step 9500, will finish Sat May  2 06:01:41 2020step 9600, will finish Sat May  2 05:59:09 2020step 9700, will finish Sat May  2 05:58:34 2020step 9800, will finish Sat May  2 05:57:32 2020step 9900, will finish Sat May  2 05:54:26 2020step 10000, will finish Sat May  2 05:51:34 2020step 10100, will finish Sat May  2 05:51:08 2020step 10200, will finish Sat May  2 05:50:13 2020step 10300, will finish Sat May  2 05:47:23 2020step 10400, will finish Sat May  2 05:45:58 2020step 10500, will finish Sat May  2 05:45:06 2020step 10600, will finish Sat May  2 05:44:12 2020step 10700, will finish Sat May  2 05:42:08 2020step 10800, will finish Sat May  2 05:39:27 2020step 10900, will finish Sat May  2 05:38:52 2020step 11000, will finish Sat May  2 05:38:12 2020step 11100, will finish Sat May  2 05:35:41 2020step 11200, will finish Sat May  2 05:33:21 2020step 11300, will finish Sat May  2 05:33:05 2020step 11400, will finish Sat May  2 05:32:26 2020step 11500, will finish Sat May  2 05:30:00 2020step 11600, will finish Sat May  2 05:28:02 2020step 11700, will finish Sat May  2 05:27:43 2020step 11800, will finish Sat May  2 05:27:46 2020step 11900, will finish Sat May  2 05:25:12 2020step 12000, will finish Sat May  2 05:23:01 2020step 12100, will finish Sat May  2 05:22:34 2020step 12200, will finish Sat May  2 05:22:29 2020step 12300, will finish Sat May  2 05:20:02 2020step 12400, will finish Sat May  2 05:19:57 2020step 12500, will finish Sat May  2 05:18:10 2020step 12600, will finish Sat May  2 05:19:48 2020step 12700, will finish Sat May  2 05:17:42 2020step 12800, will finish Sat May  2 05:17:41 2020step 12900, will finish Sat May  2 05:15:43 2020step 13000, will finish Sat May  2 05:18:10 2020step 13100, will finish Sat May  2 05:18:01 2020step 13200, will finish Sat May  2 05:17:23 2020step 13300, will finish Sat May  2 05:15:55 2020step 13400, will finish Sat May  2 05:15:42 2020step 13500, will finish Sat May  2 05:15:16 2020step 13600, will finish Sat May  2 05:14:33 2020step 13700, will finish Sat May  2 05:12:53 2020step 13800, will finish Sat May  2 05:13:32 2020step 13900, will finish Sat May  2 05:13:02 2020step 14000, will finish Sat May  2 05:13:36 2020step 14100, will finish Sat May  2 05:11:58 2020step 14200, will finish Sat May  2 05:12:31 2020step 14300, will finish Sat May  2 05:10:54 2020step 14400, will finish Sat May  2 05:12:22 2020step 14500, will finish Sat May  2 05:10:23 2020step 14600, will finish Sat May  2 05:10:06 2020step 14700, will finish Sat May  2 05:08:17 2020step 14800, will finish Sat May  2 05:09:13 2020step 14900, will finish Sat May  2 05:07:19 2020step 15000, will finish Sat May  2 05:07:01 2020step 15100, will finish Sat May  2 05:05:17 2020step 15200, will finish Sat May  2 05:06:08 2020step 15300, will finish Sat May  2 05:05:56 2020step 15400, will finish Sat May  2 05:04:16 2020step 15500, will finish Sat May  2 05:02:30 2020step 15600, will finish Sat May  2 05:03:17 2020step 15700, will finish Sat May  2 05:03:14 2020step 15800, will finish Sat May  2 05:01:44 2020step 15900, will finish Sat May  2 05:00:01 2020step 16000, will finish Sat May  2 05:01:07 2020step 16100, will finish Sat May  2 05:01:17 2020step 16200, will finish Sat May  2 04:59:35 2020step 16300, will finish Sat May  2 04:57:50 2020step 16400, will finish Sat May  2 04:59:00 2020step 16500, will finish Sat May  2 04:59:10 2020step 16600, will finish Sat May  2 04:57:31 2020step 16700, will finish Sat May  2 04:55:48 2020step 16800, will finish Sat May  2 04:57:36 2020step 16900, will finish Sat May  2 04:56:26 2020step 17000, will finish Sat May  2 04:54:58 2020[mpiexec@sgpu0502.rc.int.colorado.edu] Sending Ctrl-C to processes as requested
[mpiexec@sgpu0502.rc.int.colorado.edu] Press Ctrl-C again to force abort


Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps



Received the INT signal, stopping within 400 steps

step 17100, will finish Sat May  2 04:53:18 2020step 17200, will finish Sat May  2 04:54:30 2020step 17300, will finish Sat May  2 04:53:42 2020Ctrl-C caught... cleaning up processes
APPLICATION TERMINATED WITH THE EXIT STRING: job ending due to timeout = -1
